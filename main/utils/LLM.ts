import OpenAI from 'openai'
import { Stream } from 'openai/streaming'
import { Message } from '../../renderer/type/message'
import historyManager from './history'
import MCPManager, { IMCPConfig } from './MCP'
import { IMCPItem } from '../../renderer/type/MCP'
import filesManager from './files'
import settingManager from './setting'
import rulesManager from './rules'
import { IRule } from '../../renderer/type/rules'
import { safeParseJSON } from './common'

/** tool call ç±»å‹ */
type ToolCall = {
    id: string
    type: 'function'
    function: {
        name: string
        arguments: string
    }
}



/**
 * LLM æœåŠ¡ç±»
 * è´Ÿè´£ï¼š
 * 1. å’Œ OpenAI é€šä¿¡
 * 2. è‡ªåŠ¨å·¥å…·è°ƒç”¨ï¼ˆMCPï¼‰
 * 3. ç®¡ç†ä¸Šä¸‹æ–‡å†å²
 */
export class LLMService {
    private llm: OpenAI
    private model: string
    private toolsNameSplitString = '__'
    private defaultMaxToolRounds = 5
    private defaultContextLength = 20

    constructor(apiKey: string, baseURL: string, model: string) {
        this.llm = new OpenAI({
            apiKey,
            baseURL
        })
        this.model = model
    }

    /** MCP Tools è½¬ OpenAI Tools */
    private convertMCPToolsToOpenAITools(mcp: IMCPItem): OpenAI.Chat.Completions.ChatCompletionTool[] {
        return mcp.tools.map(tool => ({
            type: 'function',
            function: {
                name: `${mcp.id}${this.toolsNameSplitString}${tool.name}`,
                description: tool.description || '',
                parameters: tool.inputSchema || {
                    type: 'object',
                    properties: {}
                }
            }
        }))
    }

    /** è§£ææµå¼å·¥å…·è°ƒç”¨å’Œå†…å®¹è¾“å‡º */
    private async parseToolCallsFromStream(stream: Stream<OpenAI.Chat.Completions.ChatCompletionChunk>, onData: (data: string) => void): Promise<ToolCall[]> {
        const toolCalls: ToolCall[] = []
        let currentToolCall: ToolCall | null = null

        for await (const chunk of stream) {
            const choice = chunk.choices[0]

            /** å¤„ç† tool_calls */
            if (choice.delta.tool_calls) {
                const toolCallDelta = choice.delta.tool_calls[0]

                // æ–°çš„ tool call
                if (toolCallDelta.id) {
                    if (currentToolCall) {
                        toolCalls.push(currentToolCall)
                    }

                    currentToolCall = {
                        id: toolCallDelta.id,
                        type: toolCallDelta.type,
                        function: {
                            name: toolCallDelta.function?.name || '',
                            arguments: toolCallDelta.function?.arguments || ''
                        }
                    }
                }

                // arguments ç»­å†™
                else if (currentToolCall && toolCallDelta.function?.arguments) {
                    currentToolCall.function.arguments += toolCallDelta.function.arguments
                }
            }

            /** æ™®é€šæ–‡æœ¬è¾“å‡º */
            if (choice.delta.content) {
                onData(choice.delta.content)
            }

            if (choice.finish_reason) {
                if (currentToolCall) {
                    toolCalls.push(currentToolCall)
                    currentToolCall = null
                }
                break
            }
        }

        return toolCalls
    }

    /** æ‰§è¡Œ MCP å·¥å…· */
    private async executeToolCalls(mcps: IMCPItem[], toolCalls: ToolCall[]): Promise<OpenAI.Chat.Completions.ChatCompletionMessageParam[]> {
        const toolMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = []

        for (const call of toolCalls) {
            const [clientName, functionName] = call.function.name.split(this.toolsNameSplitString)
            const mcp = mcps.find(item => item.id === clientName)

            if (!mcp || !mcp.client) {
                console.error(`æ‰¾ä¸åˆ°MCPå®¢æˆ·ç«¯: ${clientName}`)
                continue
            }

            const args = safeParseJSON<{ [x: string]: unknown }>(call.function.arguments)

            try {
                const res = await mcp.client.callTool({
                    name: functionName,
                    arguments: args
                })

                console.error(`è°ƒç”¨å·¥å…·: ${functionName}`, args, res)

                toolMessages.push({
                    role: 'tool',
                    tool_call_id: call.id,
                    content: JSON.stringify(res)
                })

            } catch (error: any) {
                console.error(`è°ƒç”¨å·¥å…·å¤±è´¥ï¼Œå°è¯•é‡è¿: ${clientName}`)

                const config = safeParseJSON<IMCPConfig>(MCPManager.getConfig())
                const newClient = await MCPManager.loadSingleMCP(clientName, config.mcpServers[clientName])

                if (newClient) MCPManager.relinkClient(clientName, newClient)

                const res = await newClient?.callTool({
                    name: functionName,
                    arguments: args
                })

                toolMessages.push({
                    role: 'tool',
                    tool_call_id: call.id,
                    content: JSON.stringify(res)
                })
            }
        }

        return toolMessages
    }

    /** å¯¹å¤–ä¸»æ–¹æ³•ï¼šè¯¢é—® LLM */
    public async chat(messages: Message[], sessionId: string, files: string[], onData: (delta: string) => void): Promise<void> {
        /** å†™å…¥å†å² */
        for (const message of messages) {
            historyManager.add(sessionId, message)
        }

        const historyMessages = historyManager.getBySessionId(sessionId).messages.splice(0, 20).filter(Boolean)

        const fullResponse: Message = {
            id: Date.now(),
            role: 'assistant',
            content: '',
            time: new Date().toLocaleString(),
            isError: false
        }
        const conversation: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
            ...historyMessages,
            ...messages,
        ] as OpenAI.Chat.Completions.ChatCompletionMessageParam[]

        try {
            // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦ä¸Šä¼ æ–‡ä»¶
            if (files.length > 0) {
                conversation.push({
                    role: 'user',
                    content: `è¿™æ˜¯ç”¨æˆ·ä¼ é€’çš„æ–‡ä»¶ï¼š${filesManager.getFilesContent(files)?.join('\n')}`,
                })
            }
            // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æ·»åŠ è§„åˆ™
            const rules: IRule[] = rulesManager.loadRules() || []
            if (rules.length > 0) {
                conversation.push({
                    role: 'user',
                    content: `è¿™æ˜¯ç”¨æˆ·æ·»åŠ çš„è§„åˆ™ï¼š${rules.map(rule => rule.content).join('\n')}`,
                })
            }
            const mcps = await MCPManager.loadAll()
            const tools = mcps.flatMap(mcp => this.convertMCPToolsToOpenAITools(mcp))

            let round = 0

            while (round < settingManager.loadSettingConfig().LLM_MAX_TOOL_ROUNDS || this.defaultMaxToolRounds) {
                round++
                console.error(`ğŸ¤– Round ${round}`)

                const stream = await this.llm.chat.completions.create({
                    model: this.model,
                    messages: conversation.slice(0, settingManager.loadSettingConfig().LLM_CONTEXT_LENGTH || this.defaultContextLength) as [],  // é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                    stream: true,
                    ...(tools?.length ? { tools, tool_choice: 'auto' } : {})
                })

                const toolCalls = await this.parseToolCallsFromStream(stream, (delta) => {
                    fullResponse.content += delta
                    onData(delta)
                })

                if (toolCalls.length > 0) {
                    conversation.push({
                        role: 'assistant',
                        tool_calls: toolCalls
                    })
                    const toolMessages = await this.executeToolCalls(mcps, toolCalls)
                    conversation.push(...toolMessages)
                    continue
                }
                break
            }

            console.error('å¯¹è¯å®Œæˆ')
            historyManager.add(sessionId, fullResponse)
            filesManager.deleteAllFile()
        } catch (error) {
            const errorMessages = JSON.stringify({
                error: true,
                message: (error as Error).message || 'Unknown error'
            })
            fullResponse.isError = true
            fullResponse.content = errorMessages
            historyManager.add(sessionId, fullResponse)
            filesManager.deleteAllFile()
            throw error
        }
    }
}
