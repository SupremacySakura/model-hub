import OpenAI from 'openai'
import { Stream } from 'openai/streaming'
import { Message } from '../../renderer/type/message'
import historyManager from './history'
import MCPManager from './MCP'
import { IMCPItem } from '../../renderer/type/MCP'

/** tool call ç±»å‹ */
type ToolCall = {
    id: string
    type: 'function'
    function: {
        name: string
        arguments: string
    }
}

/**
 * LLM æœåŠ¡ç±»
 * è´Ÿè´£ï¼š
 * 1. å’Œ OpenAI é€šä¿¡
 * 2. è‡ªåŠ¨å·¥å…·è°ƒç”¨ï¼ˆMCPï¼‰
 * 3. ç®¡ç†ä¸Šä¸‹æ–‡å†å²
 */
export class LLMService {
    private llm: OpenAI
    private model: string
    private maxToolRounds = 5
    private toolsNameSplitString = '__'

    constructor(apiKey: string, baseURL: string, model: string) {
        this.llm = new OpenAI({
            apiKey,
            baseURL
        })
        this.model = model
    }

    /** MCP Tools è½¬ OpenAI Tools */
    private convertMCPToolsToOpenAITools(mcp: IMCPItem): OpenAI.Chat.Completions.ChatCompletionTool[] {
        return mcp.tools.map(tool => ({
            type: 'function',
            function: {
                name: `${mcp.id}${this.toolsNameSplitString}${tool.name}`,
                description: tool.description || '',
                parameters: tool.inputSchema || {
                    type: 'object',
                    properties: {}
                }
            }
        }))
    }

    /** è§£ææµå¼å·¥å…·è°ƒç”¨å’Œå†…å®¹è¾“å‡º */
    private async parseToolCallsFromStream(stream: Stream<OpenAI.Chat.Completions.ChatCompletionChunk>, onData: (data: string) => void): Promise<ToolCall[]> {
        const toolCalls: ToolCall[] = []
        let currentToolCall: ToolCall | null = null

        for await (const chunk of stream) {
            const choice = chunk.choices[0]

            /** å¤„ç† tool_calls */
            if (choice.delta.tool_calls) {
                const toolCallDelta = choice.delta.tool_calls[0]

                // æ–°çš„ tool call
                if (toolCallDelta.id) {
                    if (currentToolCall) {
                        toolCalls.push(currentToolCall)
                    }

                    currentToolCall = {
                        id: toolCallDelta.id,
                        type: toolCallDelta.type,
                        function: {
                            name: toolCallDelta.function?.name || '',
                            arguments: toolCallDelta.function?.arguments || ''
                        }
                    }
                }

                // arguments ç»­å†™
                else if (currentToolCall && toolCallDelta.function?.arguments) {
                    currentToolCall.function.arguments += toolCallDelta.function.arguments
                }
            }

            /** æ™®é€šæ–‡æœ¬è¾“å‡º */
            if (choice.delta.content) {
                onData(choice.delta.content)
            }

            if (choice.finish_reason) {
                if (currentToolCall) {
                    toolCalls.push(currentToolCall)
                    currentToolCall = null
                }
                break
            }
        }

        return toolCalls
    }

    /** æ‰§è¡Œ MCP å·¥å…· */
    private async executeToolCalls(mcps: IMCPItem[], toolCalls: ToolCall[]): Promise<OpenAI.Chat.Completions.ChatCompletionMessageParam[]> {
        const toolMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = []

        for (const call of toolCalls) {
            const [clientName, functionName] = call.function.name.split(this.toolsNameSplitString)
            const mcp = mcps.find(item => item.id === clientName)

            if (!mcp || !mcp.client) {
                console.error(`æ‰¾ä¸åˆ°MCPå®¢æˆ·ç«¯: ${clientName}`)
                continue
            }

            const args = JSON.parse(call.function.arguments)

            try {
                const res = await mcp.client.callTool({
                    name: functionName,
                    arguments: args
                })

                console.error(`è°ƒç”¨å·¥å…·: ${functionName}`, args, res)

                toolMessages.push({
                    role: 'tool',
                    tool_call_id: call.id,
                    content: JSON.stringify(res)
                })

            } catch (error: any) {
                console.error(`è°ƒç”¨å·¥å…·å¤±è´¥ï¼Œå°è¯•é‡è¿: ${clientName}`)

                const config = JSON.parse(MCPManager.getConfig())
                const newClient = await MCPManager.loadSingleMCP(clientName, config.mcpServers[clientName])

                if (newClient) MCPManager.relinkClient(clientName, newClient)

                const res = await newClient?.callTool({
                    name: functionName,
                    arguments: args
                })

                toolMessages.push({
                    role: 'tool',
                    tool_call_id: call.id,
                    content: JSON.stringify(res)
                })
            }
        }

        return toolMessages
    }

    /** å¯¹å¤–ä¸»æ–¹æ³•ï¼šè¯¢é—® LLM */
    public async chat(messages: Message[], sessionId: string, onData: (delta: string) => void): Promise<void> {
        /** å†™å…¥å†å² */
        for (const message of messages) {
            historyManager.add(sessionId, message)
        }

        const historyMessages = historyManager.getBySessionId(sessionId).messages.splice(0, 20).filter(Boolean)

        const fullResponse: Message = {
            id: Date.now(),
            role: 'assistant',
            content: '',
            time: new Date().toLocaleString()
        }

        const conversation: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
            ...historyMessages,
            ...messages
        ] as OpenAI.Chat.Completions.ChatCompletionMessageParam[]

        const mcps = await MCPManager.loadAll()
        const tools = mcps.flatMap(mcp => this.convertMCPToolsToOpenAITools(mcp))

        let round = 0

        while (round < this.maxToolRounds) {
            round++
            console.error(`ğŸ¤– Round ${round}`)

            const stream = await this.llm.chat.completions.create({
                model: this.model,
                messages: conversation as [],
                stream: true,
                tools,
                tool_choice: 'auto'
            })

            const toolCalls = await this.parseToolCallsFromStream(stream, (delta) => {
                fullResponse.content += delta
                onData(delta)
            })

            if (toolCalls.length > 0) {
                conversation.push({
                    role: 'assistant',
                    tool_calls: toolCalls
                })

                const toolMessages = await this.executeToolCalls(mcps, toolCalls)
                conversation.push(...toolMessages)

                continue
            }

            break
        }

        console.error('âœ… å¯¹è¯å®Œæˆ')
        console.error('fullResponse>>>', fullResponse)
        historyManager.add(sessionId, fullResponse)
    }
}
